# -*- coding: utf-8 -*-
"""MNIST_conv_dae_with_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1afGvGxGXqzdWo_VX7Z4kZes4PBi5WnKo
"""

import os

import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import save_image

from matplotlib import pyplot as plt

if not os.path.exists('./mlp_img'):
    os.mkdir('./mlp_img')


def to_img(x):
    x = x.view(x.size(0), 1, 28, 28)
    return x

def plot_sample_img(img, name):
    img = img.view(1, 28, 28)
    save_image(img, './sample_{}.png'.format(name))


def min_max_normalization(tensor, min_value, max_value):
    min_tensor = tensor.min()
    tensor = (tensor - min_tensor)
    max_tensor = tensor.max()
    tensor = tensor / max_tensor
    tensor = tensor * (max_value - min_value) + min_value
    return tensor

def add_noise(img):
    noise = torch.randn(img.size()) * 0.4
    noisy_img = img + noise
    return noisy_img

def tensor_round(tensor):
    return torch.round(tensor)

img_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),
    transforms.Lambda(lambda tensor:tensor_round(tensor))
])


batch_size = 128

dataset = MNIST('./data', transform=img_transform, download=True)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, stride=2),
            nn.ReLU(True),
            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, stride=2),
            nn.Sigmoid())

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

num_epochs = 4
batch_size = 128

model = autoencoder().cuda()
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(
    model.parameters(), lr=learning_rate, weight_decay=1e-5)

index = 0
input_tensor = dataset[index][0][0,:,:]
noisy_tensor = add_noise(input_tensor)

plt.figure()
plt.subplot(1, 2, 1)
plt.imshow(input_tensor)
plt.subplot(1, 2, 2)
plt.imshow(noisy_tensor)

model

bce_loss_list = []
mse_loss_list = []

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



plt.figure(figsize=(18, 16))
counter = -1
subplot_counter = 0
for epoch in range(num_epochs):
    print('--- epoch [{}/{}] ---'.format(epoch + 1, num_epochs))
    for data in dataloader:
        counter = counter + 1
        img, _ = data
        #print('Shape: ' + str(img.shape))
        #img = img.view(img.size(0), -1)
        noisy_img = add_noise(img)
        noisy_img = Variable(noisy_img).cuda()
        img = Variable(img).cuda()
        #print('Shape: ' + str(img.shape))
        # ===================forward=====================
        output = model(noisy_img)
        loss = criterion(output, img)
        MSE_loss = nn.MSELoss()(output, img)
        # ===================backward====================
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        mse_loss_list.append(MSE_loss.item())
        bce_loss_list.append(loss.item())

        # ===================log========================
        
        if (counter % 100 == 0):
          #if epoch % 10 == 0:
          #    x = to_img(img.cpu().data)
          #    x_hat = to_img(output.cpu().data)
          #    save_image(x, './mlp_img/x_{}.png'.format(epoch))
          #    save_image(x_hat, './mlp_img/x_hat_{}.png'.format(epoch))
          #print('Shape: ' + str(input_tensor.shape))
          output = model(input_tensor.view(1, 1, 28, 28).to(device))
          output = output.cpu().detach().numpy()
          #output = np.reshape(output_tensor.detach().numpy(), (28, 28))
          if (subplot_counter == 12):
            plt.figure(figsize=(18, 16))
            subplot_counter = 0
          
          plt.subplot(4, 3, subplot_counter + 1)
          subplot_counter = subplot_counter + 1
          plt.imshow(output[0,0,:,:])
          plt.title("Iteration " + str(counter))
          plt.colorbar()
          print('batch [{}], loss:{:.4f}, MSE_loss:{:.4f}'.format(counter + 1, loss.item(), MSE_loss.item()))

#torch.save(model.state_dict(), './sim_autoencoder.pth')

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(bce_loss_list, label='BCE Loss')
plt.title('BCE Loss')
plt.xlabel('Minibatch Number')
plt.ylabel('Loss')
plt.subplot(1, 2, 2)
plt.plot(mse_loss_list, label='MSE Loss')
plt.title('MSE Loss')
plt.xlabel('Minibatch Number')
plt.ylabel('Loss')

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(np.log10(bce_loss_list), label='BCE Loss')
plt.title('BCE Loss (log10-scale))')
plt.xlabel('Minibatch Number')
plt.ylabel('Log10 of Loss')
plt.subplot(1, 2, 2)
plt.plot(np.log10(mse_loss_list), label='MSE Loss')
plt.title('MSE Loss (log10-scale)')
plt.xlabel('Minibatch Number')
plt.ylabel('Log10 of Loss')